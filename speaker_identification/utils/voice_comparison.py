# speaker_identification/utils/voice_comparison.py

import os
import torch
import numpy as np
from pyannote.audio import Model, Inference
from pyannote.audio.pipelines import SpeakerDiarization
from pyannote.core import Segment
from scipy.spatial.distance import cosine
import logging
from django.conf import settings

logger = logging.getLogger(__name__)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
embedding_model = None
diarization_pipeline = None


def get_embedding_model():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ØµÙ…Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©"""
    global embedding_model
    if embedding_model is None:
        logger.info("Loading embedding model...")
        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ØµÙ…Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
        embedding_model = Model.from_pretrained(
            "pyannote/embedding",
            use_auth_token=os.getenv("HUGGINGFACE_TOKEN")
        )
        embedding_model = Inference(embedding_model, window="whole")
    return embedding_model


def get_diarization_pipeline():
    """ØªØ­Ù…ÙŠÙ„ pipeline Ù„Ù„Ù€ diarization"""
    global diarization_pipeline
    if diarization_pipeline is None:
        logger.info("Loading diarization pipeline...")
        diarization_pipeline = SpeakerDiarization.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=os.getenv("HUGGINGFACE_TOKEN")
        )
    return diarization_pipeline


def extract_speaker_embedding(audio_file_path):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ù…Ù† Ù…Ù„Ù ØµÙˆØªÙŠ

    Returns:
        numpy array: Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ØµÙˆØªÙŠØ© (embedding)
    """
    try:
        logger.info(f"Extracting embedding from: {audio_file_path}")

        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ embedding
        inference = get_embedding_model()

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù„Ù ÙƒØ§Ù…Ù„Ø§Ù‹
        embedding = inference(audio_file_path)

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ numpy array
        if isinstance(embedding, torch.Tensor):
            embedding = embedding.numpy()

        logger.info(f"Extracted embedding shape: {embedding.shape}")
        return embedding

    except Exception as e:
        logger.error(f"Error extracting embedding: {str(e)}")
        return None


def compare_embeddings(embedding1, embedding2):
    """
    Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ØµÙ…ØªÙŠÙ† ØµÙˆØªÙŠØªÙŠÙ†

    Returns:
        float: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (0-1)
    """
    try:
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ØµÙ…Ø§Øª numpy arrays
        if isinstance(embedding1, torch.Tensor):
            embedding1 = embedding1.numpy()
        if isinstance(embedding2, torch.Tensor):
            embedding2 = embedding2.numpy()

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (1 - cosine distance)
        similarity = 1 - cosine(embedding1.flatten(), embedding2.flatten())

        return float(similarity)

    except Exception as e:
        logger.error(f"Error comparing embeddings: {str(e)}")
        return 0.0


def save_speaker_embedding(speaker):
    """Ø­ÙØ¸ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„Ù…ØªØ­Ø¯Ø«"""
    if not speaker.reference_audio:
        logger.warning(f"Speaker {speaker.name} has no reference audio")
        return False

    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ØµÙ…Ø©
        embedding = extract_speaker_embedding(speaker.reference_audio.path)

        if embedding is not None:
            # Ø­ÙØ¸ Ø§Ù„Ø¨ØµÙ…Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            import pickle
            speaker.voice_embedding = pickle.dumps(embedding)
            speaker.save()
            logger.info(f"Saved embedding for speaker: {speaker.name}")
            return True

    except Exception as e:
        logger.error(f"Error saving embedding for {speaker.name}: {str(e)}")

    return False


def load_speaker_embedding(speaker):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„Ù…ØªØ­Ø¯Ø«"""
    if speaker.voice_embedding:
        try:
            import pickle
            embedding = pickle.loads(speaker.voice_embedding)
            return embedding
        except:
            pass

    # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø­ÙÙˆØ¸Ø©ØŒ Ø§Ø³ØªØ®Ø±Ø¬Ù‡Ø§
    if speaker.reference_audio:
        embedding = extract_speaker_embedding(speaker.reference_audio.path)
        if embedding is not None:
            save_speaker_embedding(speaker)
        return embedding

    return None


def identify_speaker_from_segment(audio_file_path, start_time, end_time):
    """
    ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØ­Ø¯Ø« Ù…Ù† Ù…Ù‚Ø·Ø¹ ØµÙˆØªÙŠ

    Args:
        audio_file_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
        start_time: ÙˆÙ‚Øª Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        end_time: ÙˆÙ‚Øª Ø§Ù„Ù†Ù‡Ø§ÙŠØ©

    Returns:
        Speaker object or None
    """
    from speaker_identification.models import Speaker

    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„Ù…Ù‚Ø·Ø¹
        inference = get_embedding_model()
        segment = Segment(start_time, end_time)
        segment_embedding = inference.crop(audio_file_path, segment)

        # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ­Ø¯Ø«ÙŠÙ†
        best_speaker = None
        best_score = 0.0
        threshold = 0.7  # Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡

        for speaker in Speaker.objects.all():
            speaker_embedding = load_speaker_embedding(speaker)
            if speaker_embedding is not None:
                similarity = compare_embeddings(segment_embedding, speaker_embedding)
                logger.info(f"Comparing with {speaker.name}: {similarity:.3f}")

                if similarity > best_score and similarity > threshold:
                    best_score = similarity
                    best_speaker = speaker

        if best_speaker:
            logger.info(f"Identified speaker: {best_speaker.name} (score: {best_score:.3f})")
        else:
            logger.warning(f"No speaker identified (best score: {best_score:.3f})")

        return best_speaker, best_score

    except Exception as e:
        logger.error(f"Error identifying speaker: {str(e)}")
        return None, 0.0


def process_meeting_with_diarization(audio_file_path):
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø¬ØªÙ…Ø§Ø¹ ÙƒØ§Ù…Ù„ Ù…Ø¹ diarization ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØ­Ø¯Ø«ÙŠÙ†

    Returns:
        list of dict: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù…Ø¹ Ø§Ù„Ù…ØªØ­Ø¯Ø«ÙŠÙ† Ø§Ù„Ù…Ø­Ø¯Ø¯ÙŠÙ†
    """
    logger.info(f"Starting diarization for: {audio_file_path}")

    try:
        # 1. ØªØ´ØºÙŠÙ„ diarization
        pipeline = get_diarization_pipeline()
        diarization = pipeline(audio_file_path)

        # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ù‚Ø·Ø¹
        segments = []
        speaker_mapping = {}  # Ø±Ø¨Ø· labels Ù…Ø¹ Ø§Ù„Ù…ØªØ­Ø¯Ø«ÙŠÙ† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠÙŠÙ†

        for turn, _, speaker_label in diarization.itertracks(yield_label=True):
            start_time = turn.start
            end_time = turn.end

            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØ­Ø¯Ø« Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡ Ø¨Ø¹Ø¯
            if speaker_label not in speaker_mapping:
                logger.info(f"Identifying speaker for label: {speaker_label}")

                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØ­Ø¯Ø« Ù…Ù† Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ØµÙˆØªÙŠØ©
                speaker, score = identify_speaker_from_segment(
                    audio_file_path,
                    start_time,
                    end_time
                )

                if speaker:
                    speaker_mapping[speaker_label] = speaker
                    logger.info(f"Mapped {speaker_label} to {speaker.name}")
                else:
                    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ­Ø¯Ø« Ø¬Ø¯ÙŠØ¯
                    from speaker_identification.models import Speaker
                    new_speaker = Speaker.objects.create(
                        name=f"Ù…ØªØ­Ø¯Ø« {speaker_label}",
                        position="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                        speaker_type="unknown"
                    )
                    speaker_mapping[speaker_label] = new_speaker

            segments.append({
                'speaker': speaker_mapping[speaker_label],
                'start': start_time,
                'end': end_time,
                'label': speaker_label
            })

        logger.info(f"Found {len(segments)} segments with {len(speaker_mapping)} speakers")
        return segments

    except Exception as e:
        logger.error(f"Error in diarization: {str(e)}")
        return []


# Ø¯Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø±
def test_voice_comparison():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…"""
    from speaker_identification.models import Speaker

    print("ğŸ”§ Testing Voice Comparison System")
    print("=" * 50)

    # 1. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ØµÙ…Ø§Øª
    print("\n1. Testing embedding extraction...")
    test_speaker = Speaker.objects.filter(reference_audio__isnull=False).first()

    if test_speaker:
        embedding = extract_speaker_embedding(test_speaker.reference_audio.path)
        if embedding is not None:
            print(f"âœ… Extracted embedding for {test_speaker.name}")
            print(f"   Shape: {embedding.shape}")

            # Ø­ÙØ¸ Ø§Ù„Ø¨ØµÙ…Ø©
            save_speaker_embedding(test_speaker)
            print(f"âœ… Saved embedding to database")
        else:
            print("âŒ Failed to extract embedding")
    else:
        print("âŒ No speakers with audio found")

    # 2. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    print("\n2. Testing embedding comparison...")
    speakers = list(Speaker.objects.filter(reference_audio__isnull=False)[:2])

    if len(speakers) >= 2:
        emb1 = load_speaker_embedding(speakers[0])
        emb2 = load_speaker_embedding(speakers[1])

        if emb1 is not None and emb2 is not None:
            similarity = compare_embeddings(emb1, emb2)
            print(f"âœ… Similarity between {speakers[0].name} and {speakers[1].name}: {similarity:.3f}")

            # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ù†ÙØ³Ù‡
            self_similarity = compare_embeddings(emb1, emb1)
            print(f"âœ… Self-similarity for {speakers[0].name}: {self_similarity:.3f}")
        else:
            print("âŒ Failed to load embeddings")

    print("\nâœ… Test completed!")


if __name__ == "__main__":
    test_voice_comparison()